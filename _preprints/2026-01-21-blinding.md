---
title: "Self-Blinding and Counterfactual Self-Simulation Mitigate Biases and Sycophancy in Large Language Models"
collection: preprints
permalink: /preprint/2026-01-21-blinding
date: 2026-01-21
venue: 'arXiv'
link: 'https://doi.org/10.48550/arXiv.2601.14553'
github: 'https://github.com/self-model/SelfBlindingLLMs'
paperurl: '/files/papers/christian2026blinding.pdf'
citation: 'Christian, B. & <b>Mazor, M.</b> (2026). Self-Blinding and Counterfactual Self-Simulation Mitigate Biases and Sycophancy in Large Language Models.'
summary_paragraph: >
   Large language models, like humans, struggle to overcome the "curse of knowledge" and ignore potentially biasing information. However, unlike humans, they possess the ability to "self-blind" by calling their own API with an appropriately redacted prompt. In this collaboration with [Brian](https://brianchristian.org/) we show that, when given the opportunity, they do just that. 
---
