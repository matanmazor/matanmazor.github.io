---
title: "Self-Blinding and Counterfactual Self-Simulation Mitigate Biases and Sycophancy in Large Language Models"
collection: preprints
permalink: /preprint/2026-01-21-blinding
date: 2026-01-21
venue: 'arXiv'
# psyarxiv: 'https://osf.io/preprints/psyarxiv/fzwu8_v3'
github: 'https://github.com/self-model/SelfBlindingLLMs'
paperurl: '/files/papers/sarna2026blinding.pdf'
citation: 'Self-Blinding and Counterfactual Self-Simulation Mitigate Biases and Sycophancy in Large Language Models.'
summary_paragraph: >
   Large language models, like humans, struggle to overcome the "curse of knowledge" and ignore potentially biasing information. However, unlike humans, they possess the ability to "self-blind" by calling their own API with an appropriately redacted prompt. In this collaboration with [Brian](https://brianchristian.org/) we show that, when given the opportunity, they do just that. 
---
